%initializeclear; clc;%load the dataX = load('weightedX.csv');y = load('weightedY.csv');m = length(X); % no of training examples%add the intercept term to X X = [ones(m,1),X];%plot the datascatter(X(:,2),y,'filled')xlabel('X'); ylabel('y');alpha = 0.01 %stepsize theta_linear = inv((X'*X))*(X'*y) %using normal equations to get the theta%plot the linear regression hypothesishold on;plot(X(:,2), X*theta_linear,'r')legend('actual Data','unweighted hypothesis')grid on; %gridhold off;tau = 0.8;[y_new,weights] = theta_weighted(X,y,alpha,tau); %theta and weights(wrt to last training example) using weighted descent%visualize local weighted regressionfigure;scatter(X(:,2),y)hold on;scatter(X(:,2),y_new,'filled')legend('Data','weighted regression(tau = 0.8)')hold off; %visualize weights function(wrt to last training example)figure;scatter(X(:,2),weights)legend('weights')xlabel('X'); ylabel('weights');%visualizing hypothesis on different values of taufigure;scatter(X(:,2),y)hold on;tau = 0.1;y_new = theta_weighted(X,y,alpha,tau);scatter(X(:,2),y_new,'filled') %case of over fitting as much more preference is giving to values very close to the corresponding inputlegend('Data','tau = 0.1')figure;scatter(X(:,2),y)hold on;tau = 0.3;y_new = theta_weighted(X,y,alpha,tau);scatter(X(:,2),y_new,'filled')legend('Data','tau = 0.3')figure;scatter(X(:,2),y)hold on;tau = 2;y_new = theta_weighted(X,y,alpha,tau);scatter(X(:,2),y_new,'filled')legend('Data','tau = 2.0')figure;scatter(X(:,2),y)hold on;tau = 10;y_new = theta_weighted(X,y,alpha,tau);scatter(X(:,2),y_new,'g','+') %case of under fitting as hypothesis is not able to get all the features of the training datalegend('Data','tau = 10.0')